{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e81859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:05:45,075 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üåê Paso 1: Scraping de B√∫squeda en ALIBABA para 'camisa' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:05:47,263 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-12-09 12:05:47,277 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-12-09 12:05:47,293 - INFO - Driver [C:\\Users\\alejo\\.wdm\\drivers\\chromedriver\\win64\\142.0.7444.175\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-12-09 12:05:48,305 - INFO - Cargando Alibaba: P√°gina 1\n",
      "2025-12-09 12:06:51,432 - INFO - P√°gina 1: 47 productos v√°lidos.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada: 47 productos encontrados.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:06:53,547 - INFO - WebDriver cerrado.\n",
      "2025-12-09 12:06:53,554 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üîé Paso 2: Scraping de Detalles para 3 Enlaces ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 12:06:55,889 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-12-09 12:06:55,903 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-12-09 12:06:55,914 - INFO - Driver [C:\\Users\\alejo\\.wdm\\drivers\\chromedriver\\win64\\142.0.7444.175\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-12-09 12:06:56,995 - INFO - Cargando detalles: https://www.alibaba.com/product-detail/Oversized-Embroidered-Striped-Shirt-Womens-Loose_1601588258560.html\n",
      "2025-12-09 12:07:10,103 - INFO - Cargando detalles: https://www.alibaba.com/product-detail/Long-sleeved-Shirt-230g-Cotton-Loose_1601638704623.html\n",
      "2025-12-09 12:07:23,473 - INFO - Cargando detalles: https://www.alibaba.com/product-detail/Unisex-Custom-Men-s-Long-Sleeve_1601463834391.html\n",
      "2025-12-09 12:07:38,748 - INFO - WebDriver cerrado.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ‚úÖ √âXITO Y EXPORTACI√ìN ---\n",
      "Datos combinados (B√∫squeda + Detalles) exportados a: alibaba_analisis_camisa_20251209_1207.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plataforma</th>\n",
       "      <th>titulo</th>\n",
       "      <th>precio</th>\n",
       "      <th>moneda</th>\n",
       "      <th>proveedor</th>\n",
       "      <th>ingresos_anuales_usd</th>\n",
       "      <th>peso_bruto_kg</th>\n",
       "      <th>precios_por_niveles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Oversized Embroidered Striped Shirt Womens Loo...</td>\n",
       "      <td>8.99</td>\n",
       "      <td>$</td>\n",
       "      <td>Dongguan Baijing Garments Co., Ltd.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'cantidad_rango': '1 - 19 pieces', 'precio_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Long-sleeved Shirt 230g Cotton Loose Round Nec...</td>\n",
       "      <td>2.97</td>\n",
       "      <td>$</td>\n",
       "      <td>Suning County Benlei Trading Store</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'cantidad_rango': '20 - 199 pieces', 'precio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Unisex Custom Men's Long Sleeve Casual Shirt E...</td>\n",
       "      <td>15.96</td>\n",
       "      <td>$</td>\n",
       "      <td>Huizhou Honglai Clothing Co., Ltd.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'cantidad_rango': '50 - 4950 pieces', 'preci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Wholesale Custom 100% Polyester Casual Blank R...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>$</td>\n",
       "      <td>RAIEBA ENTERPRISES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alibaba</td>\n",
       "      <td>Freedom Charlie Kirk Unisex T-Shirt 100% Cotto...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>$</td>\n",
       "      <td>HADEED ENTERPRISES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  plataforma                                             titulo  precio  \\\n",
       "0    Alibaba  Oversized Embroidered Striped Shirt Womens Loo...    8.99   \n",
       "1    Alibaba  Long-sleeved Shirt 230g Cotton Loose Round Nec...    2.97   \n",
       "2    Alibaba  Unisex Custom Men's Long Sleeve Casual Shirt E...   15.96   \n",
       "3    Alibaba  Wholesale Custom 100% Polyester Casual Blank R...    5.00   \n",
       "4    Alibaba  Freedom Charlie Kirk Unisex T-Shirt 100% Cotto...    4.00   \n",
       "\n",
       "  moneda                            proveedor ingresos_anuales_usd  \\\n",
       "0      $  Dongguan Baijing Garments Co., Ltd.                 None   \n",
       "1      $   Suning County Benlei Trading Store                 None   \n",
       "2      $   Huizhou Honglai Clothing Co., Ltd.                 None   \n",
       "3      $                   RAIEBA ENTERPRISES                  NaN   \n",
       "4      $                   HADEED ENTERPRISES                  NaN   \n",
       "\n",
       "  peso_bruto_kg                                precios_por_niveles  \n",
       "0          None  [{'cantidad_rango': '1 - 19 pieces', 'precio_u...  \n",
       "1          None  [{'cantidad_rango': '20 - 199 pieces', 'precio...  \n",
       "2          None  [{'cantidad_rango': '50 - 4950 pieces', 'preci...  \n",
       "3           NaN                                                NaN  \n",
       "4           NaN                                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Set\n",
    "from urllib.parse import quote_plus\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "# Dependencias de Selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException, StaleElementReferenceException,\n",
    "    WebDriverException, SessionNotCreatedException,\n",
    ")\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "# ------------------- Configuraci√≥n de Logging -------------------\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- BLOQUE BASE.PY (Se mantiene la estructura estable) ---\n",
    "\n",
    "class BaseScraper:\n",
    "    \"\"\"Configuraci√≥n com√∫n para scrapers basados en Selenium (uso local con webdriver-manager).\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str = \"data\"):\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        self._temp_dir = None\n",
    "        \n",
    "        headless_mode = False \n",
    "\n",
    "        def build_options() -> webdriver.ChromeOptions:\n",
    "            opts = webdriver.ChromeOptions()\n",
    "            \n",
    "            opts.add_argument(\"--lang=es-ES,es;q=0.9\")\n",
    "            opts.add_argument(\n",
    "                \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "            opts.add_argument(\"--no-sandbox\")\n",
    "            opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "            opts.add_argument(\"--disable-gpu\")\n",
    "            opts.add_argument(\"--window-size=1366,900\")\n",
    "            opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "            opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            \n",
    "            return opts\n",
    "\n",
    "        options = build_options()\n",
    "        \n",
    "        try:\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al iniciar Chrome: {e}\")\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            self.driver.execute_cdp_cmd(\n",
    "                \"Page.addScriptToEvaluateOnNewDocument\",\n",
    "                {\"source\": \"\"\"\n",
    "                    Object.defineProperty(navigator,'webdriver',{get:()=>undefined});\n",
    "                    window.chrome = window.chrome || {};\n",
    "                    window.chrome.app = {isInstalled: false};\n",
    "                    Object.defineProperty(navigator, 'plugins', {get: () => [1,2,3,4]});\n",
    "                    Object.defineProperty(navigator, 'mimeTypes', {get: () => [1,2,3,4]});\n",
    "                    Object.defineProperty(navigator, 'languages', {get: () => ['es-ES','es']});\n",
    "                \"\"\"}\n",
    "            )\n",
    "        except Exception: pass\n",
    "\n",
    "    def __enter__(self): return self\n",
    "    def __exit__(self, exc_type, exc, tb): self.close()\n",
    "    \n",
    "    def close(self):\n",
    "        if getattr(self, \"driver\", None):\n",
    "            try: self.driver.quit()\n",
    "            except Exception: pass\n",
    "            self.driver = None\n",
    "        if getattr(self, \"_temp_dir\", None):\n",
    "            try: self._temp_dir.cleanup()\n",
    "            except Exception: pass\n",
    "            self._temp_dir = None\n",
    "\n",
    "# --- FIN DE BASE.PY ---\n",
    "\n",
    "# --- COMIENZO DE UTILIDADES Y CLASE ALIBABA SCRAPER ---\n",
    "\n",
    "_RANGE_SPLIT_PATTERN = re.compile(r\"(?<=\\d)\\s*[-‚Äì‚Äî]\\s*(?=\\d)\")\n",
    "_PRODUCT_ID_RE = re.compile(r\"productId=(\\d+)\")\n",
    "_P4P_ID_RE = re.compile(r\"p4pid=([a-f0-9]+)\") \n",
    "_RLT_RANK_RE = re.compile(r\"rlt_rank:(\\d+)\") \n",
    "_IS_P4P_RE = re.compile(r\"is_p4p=(true|false)\")\n",
    "_COUNTRY_CODE_ATTR_RE = re.compile(r\"areaContent=(\\w{2})@@\")\n",
    "\n",
    "# Funciones de limpieza y parsing (se mantienen)\n",
    "def limpiar_precio(texto: Optional[str]) -> Optional[float]:\n",
    "    def _normalizar(texto_unitario: str) -> Optional[float]:\n",
    "        try:\n",
    "            return float(re.sub(r\"[^\\d.]\", \"\", texto_unitario))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        cleaned = re.sub(r\"[^0-9.,]\", \"\", texto_unitario)\n",
    "        if not cleaned: return None\n",
    "        if \",\" in cleaned and cleaned.rfind(\",\") > cleaned.rfind(\".\"):\n",
    "            return float(cleaned.replace('.', '').replace(',', '.'))\n",
    "        elif \".\" in cleaned and cleaned.rfind(\".\") > cleaned.rfind(\",\"):\n",
    "            return float(cleaned.replace(',', ''))\n",
    "        if re.search(r\"[\\d][.,][\\d]{1,2}$\", cleaned):\n",
    "             if cleaned.endswith(','):\n",
    "                return float(cleaned.replace('.', '').replace(',', '.'))\n",
    "             else:\n",
    "                return float(cleaned.replace(',', ''))\n",
    "        number_str = re.sub(r\"[^0-9]\", \"\", cleaned)\n",
    "        if not number_str: return None\n",
    "        try: return float(number_str)\n",
    "        except ValueError: return None\n",
    "    \n",
    "    if not texto: return None\n",
    "    texto = texto.strip(); \n",
    "    if not texto: return None\n",
    "    if re.search(r\"[-‚Äì‚Äî]\", texto):\n",
    "        partes = re.split(r\"[-‚Äì‚Äî]\", texto)\n",
    "        for p in partes:\n",
    "            if (v := _normalizar(p.strip())) is not None: return v\n",
    "        if partes: texto = partes[0]\n",
    "    return _normalizar(texto)\n",
    "\n",
    "def limpiar_cantidad(texto: Optional[str]) -> int:\n",
    "    if texto is None: return 0\n",
    "    t = texto.strip().lower().replace(\"+\", \"\")\n",
    "    if not t: return 0\n",
    "    mult = 1\n",
    "    if re.search(r\"k\\b\", t): mult = 1000; t = re.sub(r\"k\\b\", \"\", t)\n",
    "    if \"mil\" in t: mult = max(mult, 1000); t = t.replace(\"mil\", \"\")\n",
    "    n = limpiar_precio(t) or 0.0\n",
    "    if n > 1000 and \".\" in t and t.count('.') == 1 and t.split('.')[-1].isdigit():\n",
    "        n = float(t.replace('.', '')) / mult \n",
    "    return int(round(n * mult))\n",
    "\n",
    "_currency_re = re.compile(r\"(US\\$|S/|[$‚Ç¨¬£¬•]|USD)\")\n",
    "_rating_re = re.compile(r\"([\\d.]+)\\s*/\\s*5(?:\\.0)?\\s*\\((\\d+)\\)\")\n",
    "_years_re  = re.compile(r\"(\\d+)\\s*(?:a√±os|years?)\", re.I)\n",
    "\n",
    "def detectar_moneda(texto: str) -> Optional[str]:\n",
    "    if not texto: return None\n",
    "    m = _currency_re.search(texto)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def parse_rating(texto: str) -> tuple[Optional[float], Optional[int]]:\n",
    "    if not texto: return (None, None)\n",
    "    m = _rating_re.search(texto)\n",
    "    if not m: return (None, None)\n",
    "    try: return float(m.group(1)), int(m.group(2))\n",
    "    except: return (None, None)\n",
    "\n",
    "def parse_years_country(node) -> tuple[Optional[int], Optional[str]]:\n",
    "    text = \"\"; country = None\n",
    "    years = None\n",
    "    \n",
    "    try:\n",
    "        aplus_mod = node.get_attribute(\"data-aplus-auto-card-mod\") or \"\"\n",
    "        m_country = _COUNTRY_CODE_ATTR_RE.search(aplus_mod)\n",
    "        if m_country:\n",
    "            country = m_country.group(1).strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try: text = (node.text or \"\").strip()\n",
    "    except Exception: pass\n",
    "    m = _years_re.search(text)\n",
    "    if m:\n",
    "        try: years = int(m.group(1))\n",
    "        except: years = None\n",
    "        \n",
    "    if not country:\n",
    "        try:\n",
    "            img = node.find_element(By.CSS_SELECTOR, \"img[alt]\")\n",
    "            country = (img.get_attribute(\"alt\") or \"\").strip() or None\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "    return years, country\n",
    "\n",
    "def parse_moq(texto: str) -> tuple[Optional[int], Optional[str]]:\n",
    "    if not texto: return (None, None)\n",
    "    m = re.search(r\"(\\d[\\d.,]*)\", texto)\n",
    "    if not m: return (None, texto.strip())\n",
    "    try: val = limpiar_cantidad(m.group(1))\n",
    "    except: val = None\n",
    "    return val, texto.strip()\n",
    "\n",
    "\n",
    "class AlibabaScraper(BaseScraper):\n",
    "    \"\"\"Scraper Alibaba con robustez y enfoque en las 16 columnas relevantes.\"\"\"\n",
    "\n",
    "    CARD_CONTAINERS: List[str] = [\n",
    "        \"div.fy26-product-card-wrapper\", \"div.__fy26-product-card-wrapper\",\n",
    "        \"div.searchx-product-card\", \"div.card-info.gallery-card-layout-info\",\n",
    "    ]\n",
    "\n",
    "    A_CARD: List[str] = [\"h2.searchx-product-e-title a\", \"a.searchx-product-link-wrapper\", \"a\"]\n",
    "    TITLE: List[str] = [\"h2.searchx-product-e-title span\", \"h2.searchx-product-e-title a\", \"h2.search-card-e-title\"]\n",
    "    PRICE: List[str] = [\n",
    "        \"div.searchx-product-price-price-main\", \"div.searchx-product-price\", \n",
    "        \".price--two-line\", \"div[data-aplus-auto-card-mod*='area=price'] div\", \"div.price\"\n",
    "    ]\n",
    "    PRICE_ORIGINAL: List[str] = [\"del\", \"s\", \".price-origin\"]\n",
    "    MOQ_CONTAINER: List[str] = [\"div.searchx-moq\"]\n",
    "    SOLD_COUNT: List[str] = [\"div.searchx-sold-order\"]\n",
    "    SUPPLIER_NAME: List[str] = [\"a.searchx-product-e-company\", \"a.search-card-e-company\"]\n",
    "    SUPPLIER_YEAR_COUNTRY: List[str] = [\"a.searchx-product-e-supplier__year\"]\n",
    "    VERIFIED_BADGE: List[str] = [\".verified-supplier-icon__wrapper\", \"img.searchx-verified-icon\"]\n",
    "    RATING: List[str] = [\"span.searchx-product-e-review\"]\n",
    "    AD_BADGE: List[str] = [\".searchx-card-e-ad\", \"div[data-role='ad-area']\"]\n",
    "    SELLING_POINTS: List[str] = [\".searchx-selling-point-text\"]\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_text(cls, node) -> Optional[str]:\n",
    "        if node is None: return None\n",
    "        get_attribute = getattr(node, \"get_attribute\", None)\n",
    "        if callable(get_attribute):\n",
    "            content = get_attribute(\"textContent\")\n",
    "            if content: return content.strip()\n",
    "            return (getattr(node, \"text\", \"\") or \"\").strip() or None\n",
    "        return node.get_text(\" \", strip=True) or None\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_price_text(cls, node, data_attribute: Optional[str] = None) -> Optional[str]:\n",
    "        if node is None: return None\n",
    "        get_attribute = getattr(node, \"get_attribute\", None)\n",
    "        if callable(get_attribute):\n",
    "            if data_attribute:\n",
    "                v = get_attribute(data_attribute)\n",
    "                if v: return v.strip()\n",
    "            content = get_attribute(\"textContent\")\n",
    "            if content: return content.strip()\n",
    "            return (get_attribute(\"innerText\") or \"\").strip() or None\n",
    "        return node.get_text(\" \", strip=True) or None\n",
    "    \n",
    "    def _human_scroll_until_growth(self, max_scrolls: int = 16, pause: float = 1.0):\n",
    "        logging.info(\"Iniciando scroll gradual...\")\n",
    "        last_height = 0\n",
    "        scroll_count = 0\n",
    "        while scroll_count < max_scrolls:\n",
    "            try:\n",
    "                self.driver.execute_script(\"window.scrollBy(0, 700);\")\n",
    "                time.sleep(pause)\n",
    "                \n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "                if new_height == last_height:\n",
    "                    self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(pause)\n",
    "                    final_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    \n",
    "                    if final_height <= new_height:\n",
    "                        logging.info(\"Scroll gradual finalizado: No se detecta m√°s contenido.\")\n",
    "                        break\n",
    "                    last_height = final_height\n",
    "                else:\n",
    "                    last_height = new_height\n",
    "\n",
    "                scroll_count += 1\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error durante el scroll: {e}\")\n",
    "                break\n",
    "    \n",
    "    def _first_match(self, root, selectors: List[str]):\n",
    "        for css in selectors:\n",
    "            try:\n",
    "                elements = root.find_elements(By.CSS_SELECTOR, css)\n",
    "                if elements: return elements[0]\n",
    "            except Exception: continue\n",
    "        return None\n",
    "\n",
    "    def _find_all_any(self, selectors: List[str], timeout: int = 10) -> List:\n",
    "        for css in selectors:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, timeout).until(\n",
    "                    EC.visibility_of_any_elements_located((By.CSS_SELECTOR, css)) \n",
    "                )\n",
    "                els = self.driver.find_elements(By.CSS_SELECTOR, css)\n",
    "                if els: return els\n",
    "            except TimeoutException: continue\n",
    "        return []\n",
    "    \n",
    "    @staticmethod\n",
    "    def _abs_link(href: str) -> str:\n",
    "        if not href: return \"\"\n",
    "        if href.startswith(\"//\"): return \"https:\" + href\n",
    "        if href.startswith(\"/\"): return \"https://www.alibaba.com\" + href\n",
    "        if not href.startswith(\"http\"): return \"https://www.alibaba.com/\" + href\n",
    "        return href\n",
    "        \n",
    "    def _accept_banners(self, timeout: int = 5):\n",
    "        \"\"\"Intenta cerrar banners de cookies o pop-ups.\"\"\"\n",
    "        candidates = [\n",
    "            (By.XPATH, \"//button[contains(., 'Aceptar') or contains(., 'Accept')]\"),\n",
    "            (By.XPATH, \"//button[contains(., 'Allow all')]\"),\n",
    "            (By.CSS_SELECTOR, \"[role='button'][aria-label*='accept' i]\"),\n",
    "        ]\n",
    "        for by, sel in candidates:\n",
    "            try:\n",
    "                btn = WebDriverWait(self.driver, timeout).until(EC.element_to_be_clickable((by, sel)))\n",
    "                btn.click()\n",
    "                time.sleep(0.3)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def _extract_card(self, card) -> Optional[Dict]:\n",
    "        data = {}\n",
    "        # logging.info(\"-\" * 40) # Desactivamos el log de inicio para no sobrecargar la consola\n",
    "\n",
    "        try:\n",
    "            # --- Lectura Inmediata (Estrategia de Tolerancia) ---\n",
    "            aplus_data_raw = card.get_attribute(\"data-aplus-auto-offer\")\n",
    "            data[\"product_id\"] = card.get_attribute(\"data-ctrdot\")\n",
    "            \n",
    "            aplus_data = aplus_data_raw if aplus_data_raw else \"\"\n",
    "            \n",
    "            # --- FASE 1: Identificaci√≥n y Links (NO SE PUEDE FALLAR) ---\n",
    "            a = self._first_match(card, self.A_CARD) or card\n",
    "            data[\"link\"] = self._abs_link((a.get_attribute(\"href\") or \"\").strip())\n",
    "            data[\"titulo\"] = self._resolve_text(self._first_match(card, self.TITLE)) or \"Sin t√≠tulo\"\n",
    "            \n",
    "            # ID de Producto (CR√çTICO)\n",
    "            if not data.get(\"product_id\"): m = _PRODUCT_ID_RE.search(aplus_data); data[\"product_id\"] = m.group(1) if m else None\n",
    "            \n",
    "            # logging.info(f\"[DIAGN√ìSTICO] Product ID: {data['product_id']}\") # Desactivamos logs internos\n",
    "            \n",
    "            if not data[\"link\"] and data[\"titulo\"] == \"Sin t√≠tulo\": return None\n",
    "\n",
    "            # PRECIO (Extracci√≥n con m√°xima robustez)\n",
    "            price_text = None\n",
    "            for price_sel in self.PRICE:\n",
    "                price_el = self._first_match(card, [price_sel])\n",
    "                if price_el:\n",
    "                    price_text = self._resolve_price_text(price_el, \"data-price\")\n",
    "                    if price_text: break\n",
    "            \n",
    "            if not price_text:\n",
    "                 price_area_el = self._first_match(card, [\".searchx-product-price-price-main\"])\n",
    "                 if price_area_el:\n",
    "                     price_text = self._resolve_text(price_area_el)\n",
    "            \n",
    "            data[\"precio\"] = limpiar_precio(price_text)\n",
    "            data[\"moneda\"] = detectar_moneda(price_text or \"\") if price_text else None\n",
    "            \n",
    "            # Transacciones (A menudo son None si el JS falla, pero lo intentamos)\n",
    "            moq_el = self._first_match(card, self.MOQ_CONTAINER)\n",
    "            data[\"moq\"], data[\"moq_texto\"] = parse_moq(self._resolve_text(moq_el) or \"\")\n",
    "            data[\"ventas\"] = limpiar_cantidad(self._resolve_text(self._first_match(card, self.SOLD_COUNT))) \n",
    "            \n",
    "            # --- FASE 3: Proveedor y Calidad ---\n",
    "            proveedor_el = self._first_match(card, self.SUPPLIER_NAME)\n",
    "            data[\"proveedor\"] = self._resolve_text(proveedor_el)\n",
    "            data[\"proveedor_verificado\"] = bool(self._first_match(card, self.VERIFIED_BADGE))\n",
    "            \n",
    "            year_ctry_el = self._first_match(card, self.SUPPLIER_YEAR_COUNTRY)\n",
    "            data[\"proveedor_anios\"], data[\"proveedor_pais\"] = parse_years_country(year_ctry_el) if year_ctry_el else (None, None)\n",
    "            \n",
    "            rating_el = self._first_match(card, self.RATING)\n",
    "            rating_text = self._resolve_text(rating_el)\n",
    "            data[\"rating_score\"], data[\"rating_count\"] = parse_rating(rating_text or \"\")\n",
    "            \n",
    "            # --- FASE 4: Metadatos Avanzados ---\n",
    "            \n",
    "            m = _IS_P4P_RE.search(aplus_data); data[\"is_p4p\"] = (m and m.group(1) == 'true')\n",
    "            m = _RLT_RANK_RE.search(aplus_data); data[\"rlt_rank\"] = int(m.group(1)) if m and m.group(1).isdigit() else None\n",
    "            data[\"es_anuncio\"] = data.get(\"is_p4p\", False) or bool(self._first_match(card, self.AD_BADGE))\n",
    "            \n",
    "            return data\n",
    "        except (NoSuchElementException, StaleElementReferenceException): \n",
    "            # logging.warning(\"[DIAGN√ìSTICO] Tarjeta omitida por StaleElement o elemento no encontrado.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[DIAGN√ìSTICO] Error FATAL en extracci√≥n: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_blocked(driver):\n",
    "        url = (getattr(driver, \"current_url\", \"\") or \"\").lower()\n",
    "        if any(p in url for p in [\"punish\", \"robot check\", \"verify you are human\"]): return True\n",
    "        return False\n",
    "\n",
    "    def parse(self, producto: str, paginas: int = 4):\n",
    "        try:\n",
    "            resultados: List[Dict] = []\n",
    "            \n",
    "            COLUMN_ORDER = [\n",
    "                \"product_id\", \"titulo\", \"precio\", \"moneda\", \"precio_original\", \n",
    "                \"ventas\", \"moq\", \"proveedor_verificado\", \"proveedor_anios\", \"proveedor_pais\",\n",
    "                \"rating_score\", \"es_anuncio\", \"is_p4p\", \"rlt_rank\", \"link\", \n",
    "                \"fecha_scraping\"\n",
    "            ]\n",
    "\n",
    "            for page in range(1, paginas + 1):\n",
    "                q = quote_plus(producto)\n",
    "                url = f\"https://www.alibaba.com/trade/search?SearchText={q}&page={page}\"\n",
    "                logging.info(f\"Cargando Alibaba: P√°gina {page} -> {url}\")\n",
    "\n",
    "                cargada = False\n",
    "                for intento in range(3):\n",
    "                    try:\n",
    "                        self.driver.get(url)\n",
    "                        self._accept_banners(5)\n",
    "                        WebDriverWait(self.driver, 15).until(\n",
    "                            EC.visibility_of_any_elements_located((By.CSS_SELECTOR, \", \".join(self.CARD_CONTAINERS)))\n",
    "                        )\n",
    "                        self._human_scroll_until_growth(max_scrolls=16, pause=1.0)\n",
    "                        cargada = True\n",
    "                        break\n",
    "                    except (TimeoutException, WebDriverException) as e:\n",
    "                        logging.warning(f\"Reintento Alibaba p{page} ({intento + 1}): {e}\")\n",
    "                        time.sleep(1.0)\n",
    "\n",
    "                if not cargada:\n",
    "                    logging.error(f\"Omitiendo p√°gina {page} por fallos de carga.\")\n",
    "                    continue\n",
    "\n",
    "                if self._is_blocked(self.driver):\n",
    "                    logging.warning(f\"Posible bloqueo/antibot detectado en Alibaba (p√°gina {page}).\")\n",
    "\n",
    "                bloques = self._find_all_any(self.CARD_CONTAINERS, timeout=8)\n",
    "                logging.info(f\"P√°gina {page}: {len(bloques)} productos (candidatos via Selenium)\")\n",
    "\n",
    "                count_page = 0\n",
    "                for card in bloques:\n",
    "                    data = self._extract_card(card)\n",
    "                    if not data or (not data.get('link') and data.get('titulo') == 'Sin t√≠tulo'): continue\n",
    "                    \n",
    "                    final_data = {col: data.get(col) for col in COLUMN_ORDER}\n",
    "                    final_data.update({\n",
    "                        \"pagina\": page,\n",
    "                        \"plataforma\": \"Alibaba\",\n",
    "                        \"fecha_scraping\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                        \"producto_busqueda\": producto \n",
    "                    })\n",
    "                    \n",
    "                    resultados.append(final_data)\n",
    "                    count_page += 1\n",
    "\n",
    "                logging.info(f\"P√°gina {page}: {count_page} productos v√°lidos (Selenium)\")\n",
    "\n",
    "            return resultados\n",
    "        finally:\n",
    "            pass \n",
    "\n",
    "# ------------------- BLOQUE DE EJECUCI√ìN DEL NOTEBOOK (ITINERARIO) -------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # üö® LISTA DE PRODUCTOS A BUSCAR\n",
    "    PRODUCTOS_BUSQUEDA = [\n",
    "        \"CAMISA MANGA LARGA\", \"CAMISA MANGA CORTA\", \"ACCESORIOS\", \"CAMISA\", \"POLOS MANGA CORTA HOMBRE SPORT\", \n",
    "        \"PANTALON\", \"BERMUDA\", \"CORBATA\", \"SACO\", \"TERNO\", \"CALZADO\", \"CHALECO\", \"CHOMPA\", \n",
    "        \"PIJAMA\", \"CAMISA MANGA LARGA SPORT\", \"CORREA\", \"PA√ëUELO\", \"MEDIAS DEPORTIVAS\", \"CASACA\", \n",
    "        \"POLO MANGA CORTA\", \"BIKINI\", \"CAMISETA\", \"BOXER\", \"CALCETIN\", \"PULLOVER\", \"POLERA\", \n",
    "        \"ZAPATILLA\", \"CORREA TEXTIL MODA\", \"BUZO\", \"CARDIGAN\", \"ABRIGO\", \"DENIM\", \n",
    "        \"POLOS MC NI√ëO SPORT\", \"PANTALONES SPORT NI√ëO\", \"POLERA SPORT HOMBRE\", \"HOODIES\", \n",
    "        \"RELOJ\", \"ZAPATILLAS\", \"POLO SPORT MANGA LARGA\", \"ROPA DE BA√ëO\", \"ROPA DE BA√ëO\"\n",
    "    ]\n",
    "    \n",
    "    PAGINAS_A_SCRAPEAR = 10 # Se respeta la solicitud de 10 p√°ginas\n",
    "    OUTPUT_EXCEL_FILE = \"productos_alibaba_MAESTRO.xlsx\" # Archivo √∫nico de salida\n",
    "\n",
    "    todos_los_resultados = []\n",
    "    \n",
    "    logging.info(\"Iniciando Scraper de Alibaba para el itinerario completo...\")\n",
    "    \n",
    "    try:\n",
    "        scraper = AlibabaScraper() \n",
    "\n",
    "        for producto in PRODUCTOS_BUSQUEDA:\n",
    "            logging.info(f\"\\n=======================================================\")\n",
    "            logging.info(f\"== INICIANDO PRODUCTO: {producto} | {PAGINAS_A_SCRAPEAR} P√ÅGINAS ==\")\n",
    "            logging.info(f\"========================================================\")\n",
    "            \n",
    "            # Ejecuta el parseo para el producto actual\n",
    "            resultados_producto = scraper.parse(producto, PAGINAS_A_SCRAPEAR)\n",
    "            \n",
    "            if resultados_producto:\n",
    "                todos_los_resultados.extend(resultados_producto)\n",
    "\n",
    "        # 3. Convierte a DataFrame y guarda despu√©s de todos los productos\n",
    "        if todos_los_resultados:\n",
    "            df_resultados = pd.DataFrame(todos_los_resultados)\n",
    "            \n",
    "            COLUMN_ORDER_FINAL = [\n",
    "                \"producto_busqueda\", \"product_id\", \"titulo\", \"precio\", \"moneda\", \"precio_original\", \n",
    "                \"ventas\", \"moq\", \"proveedor_verificado\", \"proveedor_anios\", \"proveedor_pais\",\n",
    "                \"rating_score\", \"es_anuncio\", \"is_p4p\", \"rlt_rank\", \"link\", \n",
    "                \"fecha_scraping\", \"pagina\", \"plataforma\"\n",
    "            ]\n",
    "            \n",
    "            df_resultados = df_resultados[[col for col in COLUMN_ORDER_FINAL if col in df_resultados.columns]]\n",
    "            \n",
    "            # Exportaci√≥n a Excel (Requiere openpyxl)\n",
    "            df_resultados.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
    "            \n",
    "            logging.info(f\"\\n========================================================\")\n",
    "            logging.info(f\"‚úÖ EXTRACCI√ìN MAESTRA COMPLETADA: {len(df_resultados)} productos.\")\n",
    "            logging.info(f\"Resultados guardados en {OUTPUT_EXCEL_FILE}\")\n",
    "            logging.info(f\"========================================================\")\n",
    "            \n",
    "            print(\"\\n--- Vista Previa de los Resultados Maestros ---\")\n",
    "            print(df_resultados.head().to_markdown(index=False))\n",
    "            \n",
    "        else:\n",
    "            logging.warning(\"No se encontraron resultados v√°lidos en todo el itinerario.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ocurri√≥ un error inesperado durante el scraping: {e}\")\n",
    "    finally:\n",
    "        # 4. Cierra el driver\n",
    "        if scraper:\n",
    "            scraper.close()\n",
    "            logging.info(\"Navegador Selenium cerrado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
