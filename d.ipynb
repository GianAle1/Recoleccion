{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e81859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 11:55:47,414 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üåê Paso 1: Scraping de B√∫squeda en ALIBABA para 'camisa' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 11:55:49,492 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-12-09 11:55:49,507 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-12-09 11:55:49,522 - INFO - Driver [C:\\Users\\alejo\\.wdm\\drivers\\chromedriver\\win64\\142.0.7444.175\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-12-09 11:55:50,730 - INFO - Cargando Alibaba: P√°gina 1\n",
      "2025-12-09 11:56:39,880 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,881 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,882 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,883 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,885 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,886 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,887 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,888 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,889 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,890 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,891 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,891 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,892 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,893 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,894 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,899 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,901 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,906 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,910 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,912 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,914 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,915 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,918 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,920 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,922 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,923 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,924 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,925 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,925 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,928 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,929 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,931 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,932 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,933 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,934 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,935 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,941 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,943 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,944 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,946 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,947 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,948 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,951 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,952 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,953 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,954 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,955 - ERROR - Error extrayendo card Alibaba: 'AlibabaScraper' object has no attribute '_first_match'\n",
      "2025-12-09 11:56:39,957 - INFO - P√°gina 1: 0 productos v√°lidos.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå B√∫squeda fallida o bloqueada. No hay enlaces para detalles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 11:56:42,266 - INFO - WebDriver cerrado.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Set, Tuple, Any\n",
    "from urllib.parse import quote_plus, urljoin\n",
    "\n",
    "# Importaciones de Librer√≠as de Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, WebDriverException, StaleElementReferenceException, NoSuchElementException,\n",
    ")\n",
    "\n",
    "# WebDriver Manager\n",
    "try:\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    print(\"Instalando webdriver-manager...\")\n",
    "    subprocess.check_call(['pip', 'install', 'webdriver-manager'])\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# --- 1. CONFIGURACI√ìN GLOBAL Y UTILIDADES ---\n",
    "# ----------------------------------------------------\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0\",\n",
    "]\n",
    "BLOCK_PATTERNS = (\"punish\", \"unusual traffic\", \"error:gvs\", \"robot check\", \"are you a robot\", \"please complete the captcha\", \"slide to verify\")\n",
    "_RANGE_SPLIT_PATTERN = re.compile(r\"(?<=\\d)\\s*[-‚Äì‚Äî]\\s*(?=\\d)\")\n",
    "_currency_re = re.compile(r\"(US\\$|S/|[$‚Ç¨¬£¬•])\")\n",
    "_rating_re = re.compile(r\"([\\d.]+)\\s*/\\s*5(?:\\.0)?\\s*\\((\\d+)\\)\")\n",
    "_years_re = re.compile(r\"(\\d+)\\s*(?:a√±os|years?)\", re.I)\n",
    "_percent_re = re.compile(r\"(\\d+)\\s*%\")\n",
    "\n",
    "# Funciones de Utilidad\n",
    "def limpiar_precio(texto: Optional[str]) -> Optional[float]:\n",
    "    def _normalizar(texto_unitario: str) -> Optional[float]:\n",
    "        cleaned = re.sub(r\"[^0-9.,]\", \"\", texto_unitario)\n",
    "        if not cleaned: return None\n",
    "        decimal_sep: Optional[str] = None\n",
    "        has_dot = \".\" in cleaned; has_comma = \",\" in cleaned\n",
    "        if has_dot and has_comma: decimal_sep = \",\" if cleaned.rfind(\",\") > cleaned.rfind(\".\") else \".\"\n",
    "        elif has_dot:\n",
    "            if len(cleaned.rpartition(\".\")[-1]) in (1, 2): decimal_sep = \".\"\n",
    "        elif has_comma:\n",
    "            if len(cleaned.rpartition(\",\")[-1]) in (1, 2): decimal_sep = \",\"\n",
    "        if decimal_sep:\n",
    "            int_part, dec_part = cleaned.rsplit(decimal_sep, 1)\n",
    "            int_digits = re.sub(r\"[^0-9]\", \"\", int_part); dec_digits = re.sub(r\"[^0-9]\", \"\", dec_part)\n",
    "            number_str = f\"{int_digits}.{dec_digits or '0'}\"\n",
    "        else:\n",
    "            number_str = re.sub(r\"[^0-9]\", \"\", cleaned)\n",
    "            if not number_str: return None\n",
    "        try: return float(number_str)\n",
    "        except ValueError: return None\n",
    "    if not texto: return None\n",
    "    texto = texto.strip()\n",
    "    if _RANGE_SPLIT_PATTERN.search(texto):\n",
    "        partes = [p.strip() for p in _RANGE_SPLIT_PATTERN.split(texto) if p.strip()]\n",
    "        if partes: texto = partes[0]\n",
    "    return _normalizar(texto)\n",
    "\n",
    "def limpiar_cantidad(texto: Optional[str]) -> int:\n",
    "    if texto is None: return 0\n",
    "    t = texto.strip().lower().replace(\"+\", \"\")\n",
    "    if not t: return 0\n",
    "    mult = 1\n",
    "    if re.search(r\"k\\b\", t) or \"mil\" in t: mult = 1000; t = re.sub(r\"k\\b|mil\", \"\", t)\n",
    "    n = limpiar_precio(t) or 0.0\n",
    "    return int(round(n * mult))\n",
    "\n",
    "def detectar_moneda(texto: str) -> Optional[str]:\n",
    "    if not texto: return None\n",
    "    m = _currency_re.search(texto)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def parse_rating(texto: str) -> Tuple[Optional[float], Optional[int]]:\n",
    "    if not texto: return (None, None)\n",
    "    m = _rating_re.search(texto)\n",
    "    if not m: return (None, None)\n",
    "    try: return float(m.group(1)), int(m.group(2))\n",
    "    except: return (None, None)\n",
    "\n",
    "def parse_moq(texto: str) -> Tuple[Optional[int], Optional[str]]:\n",
    "    if not texto: return (None, None)\n",
    "    m = re.search(r\"(\\d[\\d.,]*)\", texto)\n",
    "    if not m: return (None, texto.strip())\n",
    "    try: val = limpiar_cantidad(m.group(1))\n",
    "    except: val = None\n",
    "    return val, texto.strip()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## 2. L√≥gica de Extracci√≥n de Detalles\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def extract_alibaba_product_details(driver: webdriver.Chrome, product_url: str) -> Dict[str, Any]:\n",
    "    \"\"\"Navega a una p√°gina de producto individual y extrae detalles avanzados.\"\"\"\n",
    "    \n",
    "    details: Dict[str, Any] = {\n",
    "        \"link\": product_url, \n",
    "        \"precios_por_niveles\": None, \"ingresos_anuales_usd\": None,\n",
    "        \"mercados_principales\": None, \"pais_origen_detallado\": None,\n",
    "        \"atributos_produccion\": None, \"peso_bruto_kg\": None, \"tiempos_entrega\": None\n",
    "    }\n",
    "    logging.info(f\"Cargando detalles: {product_url}\")\n",
    "    \n",
    "    try:\n",
    "        driver.get(product_url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"key-attributes\")))\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # 1. Precios por Niveles\n",
    "        ladder_prices = []; price_container = soup.select_one('div[data-testid=\"ladder-price\"]')\n",
    "        if price_container:\n",
    "            for item in price_container.select('.price-item'):\n",
    "                qty_text = item.select_one('.id-text-sm.id-text-\\\\[\\\\#666\\\\]'); price_text = item.select_one('.id-text-2xl.id-font-bold span')\n",
    "                if qty_text and price_text: ladder_prices.append({\"cantidad_rango\": qty_text.get_text(strip=True), \"precio_unitario\": limpiar_precio(price_text.get_text(strip=True))})\n",
    "        details['precios_por_niveles'] = ladder_prices\n",
    "        \n",
    "        # 2. M√©tricas de la Compa√±√≠a\n",
    "        company_data = soup.select_one('.module_unifed_company_card')\n",
    "        if company_data:\n",
    "            revenue_container = company_data.find('div', class_='id-text-sm id-truncate', string=lambda t: t and 'Ingresos anuales totales' in t)\n",
    "            if revenue_container and (revenue_val := revenue_container.find_next_sibling('div')): details['ingresos_anuales_usd'] = limpiar_precio(revenue_val.get_text(strip=True))\n",
    "            markets_container = company_data.find('div', class_='id-text-sm id-truncate', string='Mercados principales')\n",
    "            if markets_container and (markets_val := markets_container.find_next_sibling('div')): details['mercados_principales'] = markets_val.get_text(strip=True)\n",
    "            country_location_node = company_data.select_one('.id-mt-1.id-flex.id-items-center.id-gap-0\\\\.5.id-text-xs span:last-child')\n",
    "            details['pais_origen_detallado'] = country_location_node.get_text(strip=True) if country_location_node else None\n",
    "\n",
    "        # 3. Atributos Clave\n",
    "        attributes = {}; attribute_table = soup.select_one('div[data-testid=\"module-attribute\"] .id-grid-cols-2.id-border-\\\\[0\\\\.5px\\\\]')\n",
    "        if attribute_table:\n",
    "            for row in attribute_table.select('.id-grid'):\n",
    "                key_node = row.select_one('.id-bg-\\\\[\\\\#f8f8f8\\\\]'); value_node = row.select_one('.id-font-medium')\n",
    "                if key_node and value_node:\n",
    "                    key = key_node.get_text(strip=True).replace(' ', '_').lower(); value = value_node.get_text(strip=True)\n",
    "                    attributes[key] = value\n",
    "        details['atributos_produccion'] = attributes\n",
    "        details['peso_bruto_kg'] = limpiar_precio(attributes.get('peso_bruto', None)) if attributes.get('peso_bruto', None) else None\n",
    "        \n",
    "        # 4. Tiempos de Entrega\n",
    "        delivery_table = soup.select_one('.lead-list table'); delivery_times = []\n",
    "        if delivery_table:\n",
    "             rows = delivery_table.select('tbody tr')\n",
    "             if len(rows) > 1:\n",
    "                qty_ranges = [th.get_text(strip=True) for th in rows[0].select('td')]\n",
    "                time_values = [td.get_text(strip=True) for td in rows[1].select('td')]\n",
    "                for r, t in zip(qty_ranges, time_values): delivery_times.append({\"rango_cantidad\": r, \"tiempo_aprox_dias\": t})\n",
    "        details['tiempos_entrega'] = delivery_times\n",
    "        \n",
    "        return details\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al extraer detalles de {product_url}: {e}\")\n",
    "        return details\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## 4. Clase Base (BaseScraper)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "class BaseScraper:\n",
    "    \"\"\"Clase base para manejar la inicializaci√≥n y cierre del WebDriver.\"\"\"\n",
    "    \n",
    "    def __init__(self, headless: bool = True, driver_path: Optional[str] = None):\n",
    "        self.driver: Optional[webdriver.Chrome] = None; self.headless = headless; self.driver_path = driver_path; self._initialize_driver()\n",
    "    def _initialize_driver(self):\n",
    "        chrome_options = Options(); user_agent = random.choice(USER_AGENTS)\n",
    "        chrome_options.add_argument(\"--no-sandbox\"); chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\"); chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(f\"user-agent={user_agent}\"); chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"]); chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        if self.headless: chrome_options.add_argument(\"--headless\")\n",
    "        try:\n",
    "            if self.driver_path: service = ChromeService(executable_path=self.driver_path)\n",
    "            else: service = ChromeService(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        except Exception as e: logging.error(f\"Error al inicializar WebDriver: {e}. ¬øEst√° Chrome instalado?\"); raise\n",
    "    def close(self):\n",
    "        if self.driver: self.driver.quit(); self.driver = None; logging.info(\"WebDriver cerrado.\")\n",
    "    def __enter__(self): return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): self.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _abs_link(href: str, domain: str) -> str:\n",
    "        if not href: return \"\"\n",
    "        if href.startswith(\"//\"): return \"https:\" + href\n",
    "        if href.startswith(\"/\"): return urljoin(f\"https://www.{domain}.com\", href)\n",
    "        return href\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_blocked(driver) -> bool:\n",
    "        url = (getattr(driver, \"current_url\", \"\") or \"\").lower()\n",
    "        if any(p in url for p in BLOCK_PATTERNS): return True\n",
    "        html = getattr(driver, \"page_source\", \"\") or \"\"\n",
    "        try: soup = BeautifulSoup(html, \"html.parser\"); soup_text = soup.get_text(separator=\" \", strip=True).lower()\n",
    "        except Exception: soup_text = html.lower()\n",
    "        return any(p in soup_text for p in BLOCK_PATTERNS)\n",
    "    def _accept_banners(self, timeout: int = 5):\n",
    "        candidates = [(By.XPATH, \"//button[contains(., 'Aceptar') or contains(., 'Accept')]\"), (By.XPATH, \"//button[contains(., 'Allow all')]\"), (By.CSS_SELECTOR, \"[role='button'][aria-label*='accept' i]\")]\n",
    "        for by, sel in candidates:\n",
    "            try: btn = WebDriverWait(self.driver, timeout).until(EC.element_to_be_clickable((by, sel))); btn.click(); time.sleep(0.5); logging.info(\"Banner de cookies aceptado.\")\n",
    "            except Exception: continue\n",
    "    def _human_scroll_until_growth(self, max_scrolls: int = 16, pause: float = 1.0):\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\") if self.driver else 0\n",
    "        for i in range(max_scrolls):\n",
    "            if not self.driver: break\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\"); time.sleep(pause)\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height <= last_height:\n",
    "                self.driver.execute_script(\"window.scrollBy(0, 700);\"); time.sleep(pause)\n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height <= last_height: break\n",
    "            last_height = new_height\n",
    "    \n",
    "\n",
    "    \n",
    "    def _find_all_any(self, selectors: List[str], timeout: int = 10) -> List:\n",
    "        if not self.driver: return []\n",
    "        for css in selectors:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, timeout).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, css)))\n",
    "                els = self.driver.find_elements(By.CSS_SELECTOR, css);\n",
    "                if els: return els;\n",
    "            except TimeoutException: continue\n",
    "        return []\n",
    "    @staticmethod\n",
    "    def _resolve_text(node) -> Optional[str]:\n",
    "        if node is None: return None\n",
    "        get_attribute = getattr(node, \"get_attribute\", None)\n",
    "        if callable(get_attribute):\n",
    "            inner = get_attribute(\"innerText\")\n",
    "            if inner: return inner.strip()\n",
    "            return (getattr(node, \"text\", \"\") or \"\").strip() or None\n",
    "        return node.get_text(\" \", strip=True) or None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## 5. Clase AlibabaScraper \n",
    "# ----------------------------------------------------\n",
    "\n",
    "class AlibabaScraper(BaseScraper):\n",
    "    \"\"\"Scraper Alibaba.\"\"\"\n",
    "    CARD_CONTAINERS: List[str] = [\"div.fy26-product-card-content\", \"div.searchx-product-card\", \"div.card-info.gallery-card-layout-info\"]\n",
    "    A_CARD: List[str] = [\"h2.searchx-product-e-title a\", \"a.searchx-product-link-wrapper\", \"a\"]\n",
    "    TITLE: List[str] = [\"h2.searchx-product-e-title span\", \"h2.searchx-product-e-title a\", \"h2.search-card-e-title a\", \"h1, h2, h3\"]\n",
    "    PRICE: List[str] = [\"div.searchx-product-price-price-main\", \"div.searchx-product-price\", \"div.search-card-e-price-main\"]\n",
    "    PRICE_ORIGINAL: List[str] = [\"del\", \"s\", \".price-origin\"]\n",
    "    DISCOUNT: List[str] = [\".discount\", \".sale-tag\", \"[data-discount]\"]\n",
    "    SUPPLIER_NAME: List[str] = [\"a.searchx-product-e-company\", \"a.search-card-e-company\"]\n",
    "    SUPPLIER_YEAR_COUNTRY: List[str] = [\"a.searchx-product-e-supplier__year\"]\n",
    "    VERIFIED_BADGE: List[str] = [\".verified-supplier-icon__wrapper\", \"img.searchx-verified-icon\"]\n",
    "    RATING: List[str] = [\"span.searchx-product-e-review\"]\n",
    "    SELLING_POINTS: List[str] = [\".searchx-selling-point-text\"]\n",
    "\n",
    "    def _abs_link(self, href: str) -> str: return super()._abs_link(href, \"alibaba\")\n",
    "\n",
    "    def _extract_card(self, card) -> Optional[Dict]:\n",
    "        \"\"\"Extracci√≥n con try/except expl√≠cito para evitar SYNTAXERROR.\"\"\"\n",
    "        try:\n",
    "            a = self._first_match(card, self.A_CARD) or card\n",
    "            link = self._abs_link((a.get_attribute(\"href\") or \"\").strip())\n",
    "            \n",
    "            # --- Extracci√≥n de datos ---\n",
    "            titulo_el = self._first_match(card, self.TITLE); titulo = self._resolve_text(titulo_el) or \"Sin t√≠tulo\"\n",
    "            price_el = self._first_match(card, self.PRICE); price_text = self._resolve_text(price_el)\n",
    "            precio = limpiar_precio(price_text); moneda = detectar_moneda(price_text or \"\") if price_text else None\n",
    "            pori_el = self._first_match(card, self.PRICE_ORIGINAL); precio_original = limpiar_precio(self._resolve_text(pori_el) if pori_el else None)\n",
    "            desc_el = self._first_match(card, self.DISCOUNT); descuento = self._resolve_text(desc_el) if desc_el else None\n",
    "            moq_el = self._first_match(card, [\"div.searchx-moq\", \"div.price-area-center\"]); moq_val, moq_text = (None, None); \n",
    "            if moq_el: moq_text = self._resolve_text(moq_el); moq_val, _ = parse_moq(moq_text or \"\")\n",
    "            ventas = int(moq_val or 0); proveedor_el = self._first_match(card, self.SUPPLIER_NAME); proveedor = self._resolve_text(proveedor_el) if proveedor_el else None\n",
    "            year_ctry_el = self._first_match(card, self.SUPPLIER_YEAR_COUNTRY); proveedor_anios, proveedor_pais = (None, None)\n",
    "            if year_ctry_el:\n",
    "                text = (self._resolve_text(year_ctry_el) or \"\").strip(); m_years = _years_re.search(text)\n",
    "                if m_years: proveedor_anios = int(m_years.group(1)) if m_years.group(1).isdigit() else None\n",
    "                spans = year_ctry_el.find_elements(By.TAG_NAME, \"span\")\n",
    "                if spans:\n",
    "                    maybe_country = (spans[-1].text or \"\").strip()\n",
    "                    if maybe_country and len(maybe_country) <= 3: proveedor_pais = maybe_country\n",
    "            verified = bool(self._first_match(card, self.VERIFIED_BADGE)); rating_el = self._first_match(card, self.RATING); rating_score, rating_count = parse_rating(self._resolve_text(rating_el) or \"\")\n",
    "            envio_promesa = None; tasa_repeticion = None; sp = self._first_match(card, self.SELLING_POINTS)\n",
    "            if sp:\n",
    "                txt = (self._resolve_text(sp) or \"\").strip();\n",
    "                if \"env√≠o\" in txt.lower(): envio_promesa = txt\n",
    "                pr = _percent_re.search(txt);\n",
    "                if pr: tasa_repeticion = int(pr.group(1)) if pr.group(1).isdigit() else None\n",
    "\n",
    "            return {\"titulo\": titulo, \"precio\": precio, \"precio_original\": precio_original, \"descuento\": descuento, \"ventas\": ventas, \"link\": link, \"moneda\": moneda, \"proveedor\": proveedor, \"proveedor_anios\": proveedor_anios, \"proveedor_pais\": proveedor_pais, \"proveedor_verificado\": verified, \"rating_score\": rating_score, \"rating_count\": rating_count, \"moq\": moq_val, \"moq_texto\": moq_text, \"envio_promesa\": envio_promesa, \"tasa_repeticion\": tasa_repeticion,}\n",
    "        except Exception as e: \n",
    "            logging.error(f\"Error extrayendo card Alibaba: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse(self, producto: str, paginas: int = 4) -> List[Dict]:\n",
    "        resultados: List[Dict] = []; fecha_scraping = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        for page in range(1, paginas + 1):\n",
    "            q = quote_plus(producto); url = f\"https://www.alibaba.com/trade/search?SearchText={q}&page={page}\"\n",
    "            logging.info(f\"Cargando Alibaba: P√°gina {page}\")\n",
    "            cargada = False\n",
    "            for intento in range(3):\n",
    "                try:\n",
    "                    self.driver.get(url); self._accept_banners(5)\n",
    "                    WebDriverWait(self.driver, 25).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \", \".join(self.CARD_CONTAINERS))))\n",
    "                    self._human_scroll_until_growth(max_scrolls=16, pause=1.0)\n",
    "                    cargada = True; break\n",
    "                except (TimeoutException, WebDriverException) as e: logging.warning(f\"Reintento Alibaba p{page} ({intento + 1}): {e}\"); time.sleep(3.0) \n",
    "            \n",
    "            if not cargada: continue\n",
    "            if self._is_blocked(self.driver): logging.warning(\"Bloqueo de Captcha detectado (tr√°fico inusual). Deteniendo Alibaba.\"); break\n",
    "            \n",
    "            bloques = self._find_all_any(self.CARD_CONTAINERS, timeout=8); count_page = 0\n",
    "            for card in bloques:\n",
    "                data = self._extract_card(card)\n",
    "                if data:\n",
    "                    data.update({\"pagina\": page, \"plataforma\": \"Alibaba\", \"fecha_scraping\": fecha_scraping})\n",
    "                    resultados.append(data); count_page += 1\n",
    "            logging.info(f\"P√°gina {page}: {count_page} productos v√°lidos.\")\n",
    "            if count_page == 0 and page > 1: break\n",
    "        return resultados\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "## 5. Ejecuci√≥n, Consolidaci√≥n y Exportaci√≥n\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def run_alibaba_scraper_completo(producto: str, paginas_busqueda: int = 2, max_detalles: int = 3):\n",
    "    \"\"\"\n",
    "    Ejecuta el scraper de b√∫squeda y luego el scraper de detalles\n",
    "    para los primeros `max_detalles` enlaces, y exporta a CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Scraping de B√∫squeda\n",
    "    print(f\"\\n--- üåê Paso 1: Scraping de B√∫squeda en ALIBABA para '{producto}' ---\")\n",
    "    driver_path = None \n",
    "    df_lista_productos = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Modo visual activo (headless=False)\n",
    "        with AlibabaScraper(headless=False, driver_path=driver_path) as scraper:\n",
    "            resultados_lista = scraper.parse(producto, paginas=paginas_busqueda)\n",
    "            if not resultados_lista:\n",
    "                print(\"‚ùå B√∫squeda fallida o bloqueada. No hay enlaces para detalles.\")\n",
    "                return pd.DataFrame()\n",
    "            df_lista_productos = pd.DataFrame(resultados_lista)\n",
    "            print(f\"‚úÖ B√∫squeda completada: {len(df_lista_productos)} productos encontrados.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"üõë Error cr√≠tico en el scraper de b√∫squeda: {e}\"); return pd.DataFrame()\n",
    "\n",
    "    # 2. Scraping de Detalles\n",
    "    links_a_scrapear = df_lista_productos['link'].head(max_detalles).tolist()\n",
    "    print(f\"\\n--- üîé Paso 2: Scraping de Detalles para {min(len(df_lista_productos), max_detalles)} Enlaces ---\")\n",
    "    detalles_completos = []\n",
    "    \n",
    "    try:\n",
    "        with AlibabaScraper(headless=False, driver_path=driver_path) as scraper:\n",
    "            driver = scraper.driver\n",
    "            for link in links_a_scrapear:\n",
    "                detalle = extract_alibaba_product_details(driver, link) \n",
    "                detalles_completos.append(detalle)\n",
    "                time.sleep(random.uniform(2.0, 4.0)) \n",
    "    except Exception as e:\n",
    "        print(f\"üõë Error cr√≠tico durante el scraping de detalles: {e}\"); \n",
    "    \n",
    "    df_detalles = pd.DataFrame(detalles_completos)\n",
    "    \n",
    "    # 3. Consolidaci√≥n y Exportaci√≥n\n",
    "    if df_detalles.empty:\n",
    "        df_final = df_lista_productos\n",
    "        print(\"‚ö†Ô∏è No se pudieron obtener los detalles, exportando solo datos de b√∫squeda.\")\n",
    "    else:\n",
    "        df_final = pd.merge(df_lista_productos, df_detalles, on='link', how='left')\n",
    "\n",
    "    # 4. Limpieza y Exportaci√≥n Final\n",
    "    \n",
    "    nombre_archivo = f\"alibaba_analisis_{producto.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "    \n",
    "    df_final.to_csv(nombre_archivo, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n--- ‚úÖ √âXITO Y EXPORTACI√ìN ---\")\n",
    "    print(f\"Datos combinados (B√∫squeda + Detalles) exportados a: {nombre_archivo}\")\n",
    "    \n",
    "    cols_display = ['plataforma', 'titulo', 'precio', 'moneda', 'proveedor', \n",
    "                    'ingresos_anuales_usd', 'peso_bruto_kg', 'precios_por_niveles']\n",
    "    \n",
    "    cols_existentes = [col for col in cols_display if col in df_final.columns]\n",
    "    \n",
    "    display(df_final[cols_existentes].head())\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# --- EJECUCI√ìN PRINCIPAL: Buscando 'camisa' ---\n",
    "df_final = run_alibaba_scraper_completo(producto=\"camisa\", paginas_busqueda=1, max_detalles=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
